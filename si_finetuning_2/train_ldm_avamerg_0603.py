"""
U-ViT ë©€í‹°ëª¨ë‹¬ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
===========================================

ì‚¬ì „ í›ˆë ¨ëœ U-ViT ëª¨ë¸ì„ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ê¸° ìœ„í•œ
ì „ì²´ íŒŒì´í”„ë¼ì¸ (NPY ë°ì´í„°ì…‹ ì§€ì›)
"""

import os
import logging
import tempfile
from pathlib import Path
from typing import Dict, Tuple, Optional, Any, List
from dataclasses import dataclass
import copy

import torch
import torch.nn as nn
import torch.nn.functional as F
import wandb
import numpy as np
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm
import ml_collections
import accelerate
from torchvision.utils import make_grid, save_image

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ
from libs.uvit_multi_post_ln_v1 import UViT
from dpm_solver_pp import NoiseScheduleVP, DPM_Solver
from tools.fid_score import calculate_fid_given_paths
import utils
import libs.autoencoder
import torch.multiprocessing as mp


class NPYDataset(Dataset):
    """NPY íŒŒì¼ë¡œë¶€í„° ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹"""

    def __init__(self, npy_root: str, linear_proj: nn.Module = None):
        """
        NPY ë°ì´í„°ì…‹ ì´ˆê¸°í™”

        Args:
            npy_root: NPY íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            linear_proj: í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì› ë³€í™˜ìš© ì„ í˜• íˆ¬ì˜ ëª¨ë¸
        """
        self.npy_root = Path(npy_root)
        self.linear_proj = linear_proj

        # NPY íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        self.npy_files = list(self.npy_root.glob("*.npy"))
        if not self.npy_files:
            raise ValueError(f"NPY íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {npy_root}")

        logging.info(f"ì´ {len(self.npy_files)}ê°œì˜ NPY íŒŒì¼ ë°œê²¬")

        # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ë°ì´í„° í˜•íƒœ í™•ì¸
        sample_data = np.load(self.npy_files[0], allow_pickle=True).item()
        self._log_data_shapes(sample_data)

    def _log_data_shapes(self, sample_data: dict):
        """ìƒ˜í”Œ ë°ì´í„°ì˜ í˜•íƒœ ë¡œê¹…"""
        for key, value in sample_data.items():
            if isinstance(value, np.ndarray):
                logging.info(f"[ìƒ˜í”Œ] {key} shape: {value.shape}")

    def __len__(self) -> int:
        return len(self.npy_files)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ë°ì´í„° ì•„ì´í…œ ë°˜í™˜

        Returns:
            img_latent: ì´ë¯¸ì§€ ì ì¬ ë³€ìˆ˜ [4, 64, 64]
            clip_feat: CLIP ì´ë¯¸ì§€ íŠ¹ì§• [512]
            text_latent: í…ìŠ¤íŠ¸ ì ì¬ ë³€ìˆ˜ [77, 64] (linear projection ì ìš© í›„)
        """
        try:
            # NPY íŒŒì¼ ë¡œë“œ
            data = np.load(self.npy_files[idx], allow_pickle=True).item()

            # ë°ì´í„° ì¶”ì¶œ (output ë°ì´í„° ì‚¬ìš©)
            img_latent = torch.from_numpy(data['output_img_latent']).float()  # [4, 64, 64]
            clip_feat = torch.from_numpy(data['output_clip_feat']).float()    # [512]
            text_latent = torch.from_numpy(data['output_text_latent']).float() # [77, 768]

            # image latent: z-score ì •ê·œí™”
            img_latent = (img_latent - img_latent.mean()) / img_latent.std()
            # clip_feat, text_latent: L2 ì •ê·œí™”
            clip_feat = F.normalize(clip_feat, dim=-1)
            text_latent = F.normalize(text_latent, dim=-1)


            # # í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì› ë³€í™˜ (768 -> 64)
            # if self.linear_proj is not None:
            #     self.linear_proj = self.linear_proj.cpu()
            #     with torch.no_grad():
            #         text_latent = self.linear_proj(text_latent.cpu()).cpu()

            return img_latent, clip_feat, text_latent

        except Exception as e:
            logging.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (íŒŒì¼: {self.npy_files[idx]}): {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ëŒ€ì²´
            return self.__getitem__(0)


class NoiseScheduler:
    """í™•ì‚° ëª¨ë¸ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self, linear_start: float = 0.00085,
                 linear_end: float = 0.0120,
                 n_timestep: int = 1000):
        """ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        self._betas = self._create_beta_schedule(linear_start, linear_end, n_timestep)
        self.betas = np.append(0., self._betas)
        self.alphas = 1. - self.betas
        self.N = len(self._betas)

        # ìŠ¤í‚µ ì•ŒíŒŒì™€ ë² íƒ€ ê³„ì‚°
        self.skip_alphas, self.skip_betas = self._compute_skip_values()
        self.cum_alphas = self.skip_alphas[0]
        self.cum_betas = self.skip_betas[0]
        self.snr = self.cum_alphas / self.cum_betas

    def _create_beta_schedule(self, linear_start: float, linear_end: float,
                              n_timestep: int) -> np.ndarray:
        """ë² íƒ€ ìŠ¤ì¼€ì¤„ ìƒì„±"""
        return (torch.linspace(linear_start ** 0.5, linear_end ** 0.5,
                               n_timestep, dtype=torch.float64) ** 2).numpy()

    def _compute_skip_values(self) -> Tuple[np.ndarray, np.ndarray]:
        """ìŠ¤í‚µ ì•ŒíŒŒì™€ ë² íƒ€ ê°’ ê³„ì‚°"""
        N = len(self.betas) - 1
        skip_alphas = np.ones([N + 1, N + 1], dtype=self.betas.dtype)

        for s in range(N + 1):
            skip_alphas[s, s + 1:] = self.alphas[s + 1:].cumprod()

        skip_betas = np.zeros([N + 1, N + 1], dtype=self.betas.dtype)
        for t in range(N + 1):
            prod = self.betas[1: t + 1] * skip_alphas[1: t + 1, t]
            skip_betas[:t, t] = (prod[::-1].cumsum())[::-1]

        return skip_alphas, skip_betas

    def sample_multimodal(self, x_img: torch.Tensor, x_clip: torch.Tensor, y_text: torch.Tensor) -> Tuple:
        batch_size = len(x_img)

        n_img = np.random.choice(list(range(1, self.N + 1)), (batch_size,))
        n_clip = np.random.choice(list(range(1, self.N + 1)), (batch_size,))
        n_text = np.random.choice(list(range(1, self.N + 1)), (batch_size,))

        eps_img = torch.randn_like(x_img)
        eps_clip = torch.randn_like(x_clip)
        eps_text = torch.randn_like(y_text)

        xn_img = self._apply_noise(x_img, eps_img, n_img)
        xn_clip = self._apply_noise(x_clip, eps_clip, n_clip)
        yn_text = self._apply_noise(y_text, eps_text, n_text)
        # print("ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–xn_img:", xn_img.shape)
        # print("ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–xn_clip:", xn_clip.shape)
        # print("ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–xn_text:", yn_text.shape)

        return (torch.tensor(n_img, device=x_img.device),
                torch.tensor(n_clip, device=x_clip.device),
                torch.tensor(n_text, device=y_text.device)), \
            (eps_img, eps_clip, eps_text), \
            (xn_img, xn_clip, yn_text)

    def _apply_noise(self, x: torch.Tensor, eps: torch.Tensor,
                     n: np.ndarray) -> torch.Tensor:
        """í…ì„œì— ë…¸ì´ì¦ˆ ì ìš©"""
        alpha_cumprod = torch.from_numpy(self.cum_alphas[n]).type_as(x)
        beta_cumprod = torch.from_numpy(self.cum_betas[n]).type_as(x)

        # ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ ì°¨ì› ì¡°ì •
        extra_dims = (1,) * (x.dim() - 1)
        alpha_cumprod = alpha_cumprod.view(-1, *extra_dims)
        beta_cumprod = beta_cumprod.view(-1, *extra_dims)

        return (alpha_cumprod ** 0.5) * x + (beta_cumprod ** 0.5) * eps


class MultimodalLoss:
    """ë©€í‹°ëª¨ë‹¬ U-ViT ì†ì‹¤ í•¨ìˆ˜ í´ë˜ìŠ¤"""

    @staticmethod
    def compute_loss(img_latent: torch.Tensor,
                     clip_feat: torch.Tensor,
                     text_latent: torch.Tensor,
                     model: nn.Module,
                     scheduler: NoiseScheduler,
                     img_channels: int,
                     **kwargs):
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì› ë³€í™˜: [batch, 77, 768] -> [batch, 77, 64]
        if text_latent.shape[-1] == 768:
            # ì•ˆì „í•œ ì°¨ì› ë³€í™˜
            original_shape = text_latent.shape  # [batch, 77, 768]
            text_flat = text_latent.view(-1, 768)  # [batch*77, 768]
            text_projected = model.linear_proj(text_flat)  # [batch*77, 64]
            text_latent = text_projected.view(original_shape[0], original_shape[1], -1)  # [batch, 77, 64]







        # ë©€í‹°ëª¨ë‹¬ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§
        (n_img, n_clip, n_text), (eps_img, eps_clip, eps_text), (
        x_img_noised, x_clip_noised, x_text_noised) = scheduler.sample_multimodal(img_latent, clip_feat, text_latent)

        # data_type: ì–´ë–¤ modalityë¡œ í•™ìŠµí• ì§€ ê²°ì •- randintë¡œ ë¬´ì‘ìœ„ ì„ íƒ (0=img only, 1=text only, 2=multimodal)
        data_type = torch.randint(0, 3, (img_latent.size(0),), device=img_latent.device)

        # ëª¨ë¸ ì˜ˆì¸¡
        pred_img, pred_clip, pred_text = model(
            img=x_img_noised,
            clip_img=x_clip_noised,
            text=x_text_noised,
            t_img=n_img,
            t_text=n_text,
            data_type=data_type
        )

        loss_img = MultimodalLoss._mean_squared_error(eps_img - pred_img)
        loss_clip = MultimodalLoss._mean_squared_error(eps_clip - pred_clip)*0.05
        loss_text = MultimodalLoss._mean_squared_error(eps_text - pred_text)

        return loss_img, loss_clip, loss_text

    @staticmethod
    def _mean_squared_error(tensor: torch.Tensor, start_dim: int = 1) -> torch.Tensor:
        """í‰ê·  ì œê³± ì˜¤ì°¨ ê³„ì‚°"""
        return tensor.pow(2).flatten(start_dim=start_dim).mean(dim=-1)


class ModelManager:
    """ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""

    @staticmethod
    def load_uvit_model(model_path: str, config: ml_collections.ConfigDict) -> nn.Module:
        """ì‚¬ì „ í›ˆë ¨ëœ U-ViT ëª¨ë¸ ë¡œë“œ"""
        # U-ViT ëª¨ë¸ ìƒì„±
        uvit = UViT(
            img_size=config.nnet.img_size,
            in_chans=config.nnet.in_chans,
            patch_size=config.nnet.patch_size,
            embed_dim=config.nnet.embed_dim,
            depth=config.nnet.depth,
            num_heads=config.nnet.num_heads,
            mlp_ratio=config.nnet.mlp_ratio,
            qkv_bias=config.nnet.qkv_bias,
            text_dim=config.nnet.text_dim,
            num_text_tokens=config.nnet.num_text_tokens,
            clip_img_dim=config.nnet.clip_img_dim,
            use_checkpoint=config.nnet.use_checkpoint
        )

        # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location='cpu')
            state_dict = checkpoint.get('model', checkpoint)

            # token_embedding.weight í¬ê¸° ë¬¸ì œ ì²˜ë¦¬ -
            if 'token_embedding.weight' in state_dict:
                old_emb = state_dict['token_embedding.weight']      # torch.Size([2, 1536]) - error ë°œìƒ
                expected_shape = uvit.token_embedding.weight.shape  # torch.Size([3, 1536])
                if old_emb.shape != expected_shape:
                    print(f"token_embedding.weight í¬ê¸° mismatch: checkpoint {old_emb.shape} -> model {expected_shape}")
                    # ìƒˆ ì„ë² ë”© í…ì„œ ì´ˆê¸°í™”
                    new_emb = torch.zeros(expected_shape)
                    # ê¸°ì¡´ ì„ë² ë”© weight ë³µì‚¬ (ê°€ëŠ¥í•œ ë§Œí¼)
                    n_copy = min(old_emb.shape[0], expected_shape[0])
                    new_emb[:n_copy] = old_emb[:n_copy]
                    # ë‚˜ë¨¸ì§€ ì„ë² ë”©ì€ ëœë¤ ì´ˆê¸°í™” (í‘œì¤€í¸ì°¨ 0.02)
                    if expected_shape[0] > n_copy:
                        new_emb[n_copy:] = torch.randn(expected_shape[0] - n_copy, expected_shape[1]) * 0.02

                    state_dict['token_embedding.weight'] = new_emb

            uvit.load_state_dict(state_dict, strict=False)
            logging.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        else:
            logging.warning(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        # ì„ íƒì  ëª¨ë¸ ê³ ì •
        ModelManager._freeze_model_layers(uvit)

        return ModelManager.UViTWrapper(uvit)

    @staticmethod
    def _freeze_model_layers(model: nn.Module) -> None:
        """ë©”ëª¨ë¦¬ ì ˆì•½, finetuningì„ ìœ„í•œ ì„ íƒì  ë ˆì´ì–´ ê³ ì •"""
        total_params = 0
        frozen_params = 0

        for name, param in model.named_parameters():
            total_params += param.numel()

            # # ì¶œë ¥ ë ˆì´ì–´ì™€ ë§ˆì§€ë§‰ 3ê°œ ë¸”ë¡ë§Œ í•™ìŠµ
            # if any(keyword in name for keyword in [
            #     'decoder_pred', 'clip_img_out', 'text_out',
            #     'blocks.29', 'blocks.28', 'blocks.27'  # depth=30ì´ë¯€ë¡œ 29, 28, 27
            # ]):
            # í•™ìŠµí•  ë ˆì´ì–´ ì„ íƒ
            if any(keyword in name for keyword in [
                'decoder_pred', 'clip_img_out', 'text_out',
                'mid_block',
                'out_blocks.13', 'out_blocks.14',
                'in_blocks.13', 'in_blocks.14',
                'token_embedding'
            ]):
                param.requires_grad = True
            else:
                param.requires_grad = False
                frozen_params += param.numel()

            # layer grad ê³ ì • í™•ì¸
            if param.requires_grad:
                if param.grad is not None:
                    print(f"[âœ…] {name} has grad with mean={param.grad.abs().mean():.6f}")
                else:
                    print(f"[âŒ] {name} has NO grad!")

        freeze_ratio = frozen_params / total_params * 100
        logging.info(f"ëª¨ë¸ ê³ ì • ì™„ë£Œ: {frozen_params}/{total_params} "
                     f"íŒŒë¼ë¯¸í„° ({freeze_ratio:.1f}%)")

    class UViTWrapper(nn.Module):
        """U-ViT ëª¨ë¸ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤"""

        def __init__(self, uvit_model: nn.Module):
            super().__init__()
            self.uvit = uvit_model
            self.num_text_tokens = uvit_model.num_text_tokens
            self.embed_dim = uvit_model.embed_dim
            self.linear_proj = nn.Linear(768, 64)   # í•™ìŠµ ê°€ëŠ¥í•œ text linear proj

        def forward(self, img: torch.Tensor, clip_img: torch.Tensor,
                    text: torch.Tensor, t_img: torch.Tensor,
                    t_text: torch.Tensor, data_type: torch.Tensor) -> Tuple:
            """ëª¨ë¸ ìˆœì „íŒŒ"""
            return self.uvit(img, clip_img, text, t_img, t_text, data_type)


class TrainingManager:
    """í›ˆë ¨ ê³¼ì • ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, config: ml_collections.ConfigDict):
        """í›ˆë ¨ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.config = config
        self.accelerator = accelerate.Accelerator(mixed_precision=config.mixed_precision)
        self.device = self.accelerator.device
        accelerate.utils.set_seed(config.seed, device_specific=True)
        logging.info(f'Process {self.accelerator.process_index} using device: {self.device}')
        self.scheduler = NoiseScheduler()
        self.setup_logging()

    def setup_logging(self) -> None:
        """ë¡œê¹… ì„¤ì •"""
        if self.accelerator.is_main_process:
            # wandb ì´ˆê¸°í™”
            wandb.init(
                dir=os.path.join('workdir'),
                project=f'uvit_finetune_{self.config.dataset.name}',
                config=self.config.to_dict(),
                name='finetune_avamerg',
                job_type='finetune',
                mode='online'
            )

            # ë¡œê±° ì„¤ì •
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler('workdir/training.log'),
                    logging.StreamHandler()
                ]
            )
            logging.info("í›ˆë ¨ ì„¤ì •:")
            logging.info(self.config)
        else:
            logging.basicConfig(level=logging.ERROR)

    def setup_data_loader(self) -> DataLoader:
        """ë°ì´í„° ë¡œë” ì„¤ì •"""
        try:
            # Linear projection ëª¨ë¸ ìƒì„± (768 -> 64 ì°¨ì› ë³€í™˜)
            linear_proj = nn.Linear(768, 64).to(self.device)

            # NPY ë°ì´í„°ì…‹ ìƒì„±
            dataset = NPYDataset(
                npy_root=self.config.dataset.train_npy_root,
                # linear_proj=linear_proj
                linear_proj=None  # UViTWrapperì—ì„œ ì •ì˜í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ëŠ” ì œê±°í•¨
            )

            return DataLoader(
                dataset,
                batch_size=self.config.train.batch_size,
                shuffle=True,
                drop_last=True,
                num_workers=4,
                pin_memory=True,
                persistent_workers=True,
            )

        except Exception as e:
            logging.error(f"ë°ì´í„° ë¡œë” ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def train_step(self, batch: Tuple, model: nn.Module,
                   optimizer: torch.optim.Optimizer) -> Dict[str, float]:
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í… ìˆ˜í–‰"""
        metrics = {}
        optimizer.zero_grad()

        try:
            img_latents, clip_features, text_features = batch
            img_latents = img_latents.to(self.device)
            clip_features = clip_features.to(self.device)
            text_features = text_features.to(self.device)

            # ë©€í‹°ëª¨ë‹¬ ì†ì‹¤ ê³„ì‚°
            with self.accelerator.autocast():
                loss_img, loss_clip, loss_text = MultimodalLoss.compute_loss(
                    img_latent=img_latents,
                    clip_feat=clip_features,
                    text_latent=text_features,
                    model=model,
                    scheduler=self.scheduler,
                    img_channels=img_latents.shape[1]
                )
            loss = loss_img + loss_clip + loss_text

            # ì—­ì „íŒŒ
            self.accelerator.backward(loss.mean())

            for name, param in model.named_parameters():
                if param.requires_grad:
                    if param.grad is not None:
                        print(f"[â­â­â­âœ…] {name} has grad with mean={param.grad.abs().mean():.6f}")
                    else:
                        print(f"[â­â­â­âŒ] {name} has NO grad!")




            # âœ… wandbìš© metrics ì •ë¦¬
            metrics['loss'] = self.accelerator.gather(loss.detach()).mean().item()
            metrics['loss_img'] = self.accelerator.gather(loss_img.detach()).mean().item()
            metrics['loss_clip'] = self.accelerator.gather(loss_clip.detach()).mean().item()
            metrics['loss_text'] = self.accelerator.gather(loss_text.detach()).mean().item()

            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            if hasattr(self.config.train, 'grad_clip') and self.config.train.grad_clip > 0:
                self.accelerator.clip_grad_norm_(
                    model.parameters(),
                    self.config.train.grad_clip
                )

            optimizer.step()
            metrics['loss'] = self.accelerator.gather(loss.detach()).mean().item()

        except Exception as e:
            logging.error(f"í›ˆë ¨ ìŠ¤í… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        return metrics

    def _reshape_clip_features(self, clip_features: torch.Tensor,
                               img_latents: torch.Tensor) -> torch.Tensor:
        """CLIP íŠ¹ì§•ì„ ì´ë¯¸ì§€ ì ì¬ ë³€ìˆ˜ì™€ ê°™ì€ ê³µê°„ ì°¨ì›ìœ¼ë¡œ ë³€í™˜"""
        # [B, D] â†’ [B, D, 1, 1] â†’ [B, D, H, W]
        clip_features = clip_features.unsqueeze(-1).unsqueeze(-1)
        clip_features = clip_features.expand(-1, -1, img_latents.shape[2], img_latents.shape[3])
        return clip_features

    @staticmethod
    def update_ema(ema_model: nn.Module, model: nn.Module,
                   decay: float = 0.9999) -> None:
        """ì§€ìˆ˜ ì´ë™ í‰ê· (EMA) ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        with torch.no_grad():
            for ema_param, param in zip(ema_model.parameters(), model.parameters()):
                ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)

    def save_checkpoint(self, model: nn.Module, ema_model: nn.Module,
                       optimizer: torch.optim.Optimizer, step: int) -> None:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if self.accelerator.is_main_process:
            checkpoint = {
                'model': self.accelerator.unwrap_model(model).state_dict(),
                'ema_model': self.accelerator.unwrap_model(ema_model).state_dict(),
                'optimizer': optimizer.state_dict(),
                'step': step,
                'config': self.config.to_dict()
            }
            save_path = f'checkpoint_step_{step}.pth'
            torch.save(checkpoint, save_path)
            logging.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {save_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ì„¤ì • ë¡œë“œ (import ê²½ë¡œ ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
        logging.info(f"get_config ì§„ì…")
        from configs.finetune_uvit_config import get_config
        config = get_config()
        logging.info(f"TrainingManager ì§„ì…")
        # í›ˆë ¨ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        trainer = TrainingManager(config)
        logging.info(f"data_loader, load_uvit_model ì§„ì…")
        # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
        data_loader = trainer.setup_data_loader()
        model = ModelManager.load_uvit_model(config.nnet.pretrained_path, config)
        logging.info(f"ema_model ì§„ì…")
        # EMA ëª¨ë¸ ìƒì„±
        ema_model = ModelManager.load_uvit_model(config.nnet.pretrained_path, config)
        ema_model.eval()
        logging.info(f"ì˜µí‹°ë§ˆì´ì € ì„¤ì •, trainable_params ì§„ì…")
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        #print(f"â­â­â­â­Trainable parameters: {trainable_params}")
        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=config.optimizer.lr,
            weight_decay=config.optimizer.weight_decay,
            betas=config.optimizer.betas
        )
        logging.info(f"lr_scheduler ì§„ì…")
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lambda step: min(step / config.lr_scheduler.warmup_steps, 1.0)
        )
        logging.info(f"ë¶„ì‚°í•™ìŠµì¤€ë¹„ ì§„ì…")
        # ë¶„ì‚° í›ˆë ¨ ì¤€ë¹„
        model, ema_model, optimizer, data_loader, lr_scheduler = trainer.accelerator.prepare(
            model, ema_model, optimizer, data_loader, lr_scheduler
        )

        # í›ˆë ¨ ì‹œì‘
        logging.info("U-ViT íŒŒì¸íŠœë‹ ì‹œì‘")
        step = 0
        model.train()

        while step < config.train.n_steps:
            for batch in data_loader:
                if step >= config.train.n_steps:
                    break

                # í›ˆë ¨ ìŠ¤í… ìˆ˜í–‰
                metrics = trainer.train_step(batch, model, optimizer)

                # EMA ì—…ë°ì´íŠ¸
                TrainingManager.update_ema(ema_model, model, config.get('ema_rate', 0.9999))

                # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
                lr_scheduler.step()

                # ë¡œê¹…
                if trainer.accelerator.is_main_process and step % config.train.log_interval == 0:
                    metrics['lr'] = optimizer.param_groups[0]['lr']
                    metrics['step'] = step
                    logging.info(f"Step {step}: Loss={metrics['loss']:.6f}, loss_img={metrics['loss_img']:.4f}, "
                                 f"loss_clip={metrics['loss_clip']:.4f}, loss_text={metrics['loss_text']:.4f}, "
                                 f"LR={metrics['lr']:.7f}")
                    wandb.log(metrics, step=step)

                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if step % config.train.save_interval == 0 and step > 0:
                    trainer.save_checkpoint(model, ema_model, optimizer, step)

                step += 1

        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        trainer.save_checkpoint(model, ema_model, optimizer, step)
        logging.info("í›ˆë ¨ ì™„ë£Œ!")

    except Exception as e:
        logging.error(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format=f"%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),  # ì½˜ì†” ì¶œë ¥
            logging.FileHandler("train.log")  # íŒŒì¼ ë¡œê·¸
        ]
    )
    if torch.distributed.is_initialized():
        rank = torch.distributed.get_rank()
        logging.info(f"Process {rank} started.")
    else:
        logging.info("Single-process start.")
    logging.info("mp ì§„ì…")
    import sys
    sys.stdout.flush()
    mp.set_start_method("spawn", force=True)
    logging.info("main ì§„ì…")
    main()
