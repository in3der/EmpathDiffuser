import ml_collections
import torch
import random
import utils
from dpm_solver_pp import NoiseScheduleVP, DPM_Solver
from absl import logging
import einops
import libs.autoencoder
import libs.clip
from torchvision.utils import save_image, make_grid
import torchvision.transforms as standard_transforms
import numpy as np
import clip
from PIL import Image
import time


def stable_diffusion_beta_schedule(linear_start=0.00085, linear_end=0.0120, n_timestep=1000):
    _betas = (
        torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2
    )
    return _betas.numpy()


def prepare_contexts(config, clip_text_model, clip_img_model, clip_img_model_preprocess, autoencoder):
    import einops
    import numpy as np
    import torch
    import utils  # ìœ í‹¸ í•¨ìˆ˜ì— center_crop í•¨ìˆ˜ê°€ ìˆì–´ì•¼ í•¨
    from PIL import Image

    resolution = config.z_shape[-1] * 8
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    def get_img_feature(image):
        image = np.array(image).astype(np.uint8)
        image = utils.center_crop(resolution, resolution, image)

        clip_img_feature = clip_img_model.encode_image(
            clip_img_model_preprocess(Image.fromarray(image)).unsqueeze(0).to(device)
        )

        image = (image / 127.5 - 1.0).astype(np.float32)
        image = einops.rearrange(image, 'h w c -> 1 c h w')
        image = torch.tensor(image, device=device)

        moments = autoencoder.encode_moments(image)
        return clip_img_feature, moments

    # ì´ˆê¸°ê°’ (fallback ìš©ë„)
    contexts = torch.randn(config.n_samples, 77, config.clip_text_dim).to(device)
    img_contexts = torch.randn(config.n_samples, 2 * config.z_shape[0], config.z_shape[1], config.z_shape[2]).to(device)
    clip_imgs = torch.randn(config.n_samples, 1, config.clip_img_dim).to(device)

    if config.mode in ['t2i', 't2i2t']:
        prompts = [config.prompt] * config.n_samples
        contexts = clip_text_model.encode(prompts)

    elif config.mode in ['i2t', 'i2t2i']:
        image = Image.open(config.img).convert('RGB')
        clip_img, img_context = get_img_feature(image)
        img_contexts = img_context.repeat(config.n_samples, 1, 1, 1)
        clip_imgs = clip_img.repeat(config.n_samples, 1, 1)

    elif config.mode == 'joint':
        prompts = [config.prompt] * config.n_samples
        contexts = clip_text_model.encode(prompts)

        image = Image.open(config.img).convert('RGB')
        clip_img, img_context = get_img_feature(image)

        img_contexts = img_context.repeat(config.n_samples, 1, 1, 1)
        clip_imgs = clip_img.repeat(config.n_samples, 1, 1)

    return contexts, img_contexts, clip_imgs





def unpreprocess(v):  # to B C H W and [0, 1]
    v = 0.5 * (v + 1.)
    v.clamp_(0., 1.)
    return v


def set_seed(seed: int):        # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ëœë¤ ì‹œë“œ ê³ ì •
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def evaluate(config):
    # PyTorchì˜ CUDNN ìµœì í™” ì„¤ì • (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì„ íƒì  ì„¤ì •)
    if config.get('benchmark', False):
        torch.backends.cudnn.benchmark = True  # ì…ë ¥ í¬ê¸°ê°€ ê³ ì •ì¼ ë•Œ ì„±ëŠ¥ ìµœì í™”
        torch.backends.cudnn.deterministic = False  # ê²°ê³¼ ì¬í˜„ì„±ë³´ë‹¤ ì†ë„ ìš°ì„ 

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    set_seed(config.seed)

    # ì„¤ì •ì„ ë¶ˆë³€ìœ¼ë¡œ ê³ ì • (ìˆ˜ì • ë°©ì§€)
    config = ml_collections.FrozenConfigDict(config)

    # ë¡œê¹… ì´ˆê¸°í™”
    utils.set_logger(log_level='info')

    # ìŠ¤ì¼€ì¤„ í•¨ìˆ˜ë¡œ beta ê°’ ì´ˆê¸°í™” (diffusion noise level ì¡°ì ˆìš©)
    _betas = stable_diffusion_beta_schedule()
    N = len(_betas)  # íƒ€ì„ìŠ¤í… ê°œìˆ˜

    # â­ë©”ì¸ ëª¨ë¸ ë¡œë”© (U-ViT)
    nnet = utils.get_nnet(**config.nnet)    # libs. name='uvit_multi_post_ln_v1',
    logging.info(f'load nnet from {config.nnet_path}')

    # U-ViTì— ì €ì¥ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    #nnet.load_state_dict(torch.load(config.nnet_path, map_location='cpu'))
    # checkpointì—ì„œ 'model' í‚¤ë§Œ êº¼ëƒ„
    checkpoint = torch.load(config.nnet_path, map_location='cpu')
    state_dict = checkpoint.get('model', checkpoint)


    USE_LINEAR_PROJ = True  # â­ ì´ ê°’ì„ Falseë¡œ ë°”ê¾¸ë©´ linear_proj ì•ˆ ì”€

    def strip_prefix_if_present(state_dict, prefix="uvit."):
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith(prefix):
                new_key = k[len(prefix):]
            else:
                new_key = k
            new_state_dict[new_key] = v
        return new_state_dict

    # prefix strip (ìë™ íŒë‹¨)
    if any(k.startswith('uvit.') for k in state_dict.keys()):
        state_dict = strip_prefix_if_present(state_dict, prefix='uvit.')

    # linear_proj ë”°ë¡œ pop
    linear_proj_state = {
        'weight': state_dict.pop('linear_proj.weight', None),
        'bias': state_dict.pop('linear_proj.bias', None)
    }


    # token_embedding í¬ê¸° ë§ì¶”ê¸° (í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ embedding êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì¤Œ)
    # old_token_emb = state_dict.get('token_embedding.weight')  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê¸°ì¡´ token_embedding.weight ë¶ˆëŸ¬ì˜¤ê¸°
    # print("ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹ğŸ¥¹", old_token_emb.shape)
    # if old_token_emb is not None:
    #     new_shape = nnet.state_dict()['token_embedding.weight'].shape  # í˜„ì¬ ëª¨ë¸ì˜ token_embedding ê¸°ëŒ€ shape í™•ì¸
    #     print("new_shape : ", new_shape)
    #
    #     # ìƒˆë¡œìš´ embedding weight í…ì„œ ì´ˆê¸°í™” (ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” shapeìœ¼ë¡œ)
    #     new_token_emb = torch.zeros(new_shape)
    #
    #     # ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ ê¸°ì¡´ ì„ë² ë”© ë³µì‚¬ (ì˜ˆ: old = [2, 1536], new = [3, 1536] â†’ 2ê°œ ë³µì‚¬)
    #     n_copy = min(old_token_emb.shape[0], new_shape[0])
    #     new_token_emb[:n_copy] = old_token_emb[:n_copy]
    #
    #     # ë³µì‚¬ í›„ ë‚¨ì€ ë¶€ë¶„ì€ ëœë¤ ì´ˆê¸°í™” (í•™ìŠµ ì‹œ ë°©ì‹ê³¼ ë™ì¼, std=0.02)
    #     if new_shape[0] > n_copy:
    #         new_token_emb[n_copy:] = torch.randn(new_shape[0] - n_copy, new_shape[1]) * 0.02
    #
    #     # ìˆ˜ì •ëœ token_embeddingìœ¼ë¡œ state_dict ê°±ì‹ 
    #     state_dict['token_embedding.weight'] = new_token_emb
    #     print("new_token_emb : ", new_token_emb)
    # tokenê°œìˆ˜ 3ìœ¼ë¡œ ë§ì¶˜ ë²„ì „
    # token_embedding í¬ê¸° ë§ì¶”ê¸° (checkpointì—ì„œëŠ” [3, 1536], í˜„ì¬ ëª¨ë¸ì€ [2, 1536]ì„ ê¸°ëŒ€í•  ë•Œ)
    # token_embedding í¬ê¸° í™•ì¸ (í•„ìš” ì‹œ resize)
    old_token_emb = state_dict.get('token_embedding.weight')
    expected_shape = nnet.token_embedding.weight.shape

    if old_token_emb is not None and old_token_emb.shape != expected_shape:
        print(f"token_embedding í¬ê¸° mismatch: {old_token_emb.shape} -> {expected_shape}")
        new_token_emb = torch.zeros(expected_shape)
        n_copy = min(old_token_emb.shape[0], expected_shape[0])
        new_token_emb[:n_copy] = old_token_emb[:n_copy]
        if expected_shape[0] > n_copy:
            new_token_emb[n_copy:] = torch.randn(expected_shape[0] - n_copy, expected_shape[1]) * 0.02
        state_dict['token_embedding.weight'] = new_token_emb

    # ëª¨ë¸ì— ë¡œë“œ
    try:
        nnet.load_state_dict(state_dict, strict=True)
    except RuntimeError as e:
        print("Error loading state dict:", e)

    # # prefix ë¬¸ì œ ì²˜ë¦¬ (í•„ìš” ì‹œ)
    # def strip_prefix_if_present(state_dict, prefix="uvit."):
    #     new_state_dict = {}
    #     for k, v in state_dict.items():
    #         if k.startswith(prefix):
    #             new_key = k[len(prefix):]
    #         else:
    #             new_key = k
    #         new_state_dict[new_key] = v
    #     return new_state_dict
    #
    # state_dict = strip_prefix_if_present(state_dict, prefix="uvit.")
    #
    # # linear_proj weight/biasëŠ” ë”°ë¡œ ë¶„ë¦¬ (nnetì— í¬í•¨ì‹œí‚¤ë©´ ì•ˆë¨)
    # linear_proj_state = {
    #     'weight': state_dict.pop('linear_proj.weight', None),
    #     'bias': state_dict.pop('linear_proj.bias', None)
    # }
    #
    # # token_embedding í¬ê¸° ë§ì¶”ê¸°
    # old_token_emb = state_dict.get('token_embedding.weight')
    # if old_token_emb is not None:
    #     new_shape = nnet.state_dict()['token_embedding.weight'].shape
    #
    #     def resize_token_embedding(old_tensor, new_shape, mean_resizing=True):
    #         if old_tensor.shape == new_shape:
    #             return old_tensor
    #         if mean_resizing:
    #             mean = old_tensor.mean(dim=0, keepdim=True)
    #             return mean.repeat(new_shape[0], 1)
    #         else:
    #             return torch.zeros(new_shape)
    #
    #     state_dict['token_embedding.weight'] = resize_token_embedding(old_token_emb, new_shape)
    #
    # # U-ViTì— state_dict ë¡œë“œ
    # try:
    #     nnet.load_state_dict(state_dict, strict=True)
    # except RuntimeError as e:
    #     print("Error loading state dict:", e)

    nnet.to(device)
    nnet.eval()

    # â­ linear_proj ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
    if USE_LINEAR_PROJ:
        linear_proj = torch.nn.Linear(768, 64)
        if linear_proj_state['weight'] is not None and linear_proj_state['bias'] is not None:
            linear_proj.load_state_dict(linear_proj_state)
            print("âœ… linear_proj loaded")
        else:
            print("âš ï¸ linear_proj weights not found, using randomly initialized layer")
        linear_proj.to(device)
        linear_proj.eval()
    else:
        linear_proj = None
        print("â„¹ï¸ linear_proj disabled")

    ## í•„ìš”í•œ ê²½ìš° shape mismatch ì²˜ë¦¬ë„ ê°€ëŠ¥
    #nnet.load_state_dict(state_dict, strict=False)
#
    ## â­í…ìŠ¤íŠ¸ ì„ë² ë”© í”„ë¡œì ì…˜ (í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ êµ¬ì¡°)
    #linear_proj = torch.nn.Linear(768, 64)
    #linear_proj.load_state_dict({  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ weight ë¶ˆëŸ¬ì˜¤ê¸°
    #    'weight': state_dict['linear_proj.weight'],
    #    'bias': state_dict['linear_proj.bias']
    #})
    #linear_proj.to(device)
    #linear_proj.eval()
#
    #nnet.to(device)
    #nnet.eval()

    # ìº¡ì…˜ ë””ì½”ë” ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    # ì¡°ê±´: í…ìŠ¤íŠ¸ ì°¨ì›ì´ ì‘ê±°ë‚˜, í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€(T2I) ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°
    use_caption_decoder = config.text_dim < config.clip_text_dim or config.mode != 't2i'
    if use_caption_decoder:
        # ìº¡ì…˜ ë””ì½”ë” ì´ˆê¸°í™” (ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ ë“±ì—ì„œ ì‚¬ìš©) - í…ìŠ¤íŠ¸ ìƒì„±í•˜ëŠ” ë””ì½”ë”
        from libs.caption_decoder import CaptionDecoder
        caption_decoder = CaptionDecoder(device=device, **config.caption_decoder)
    else:
        caption_decoder = None  # ì‚¬ìš© ì•ˆí•¨

    # â­CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¡œë”© (ê³ ì •ëœ ì‚¬ì „í•™ìŠµ ëª¨ë¸)
    clip_text_model = libs.clip.FrozenCLIPEmbedder(device=device)
    clip_text_model.eval()
    clip_text_model.to(device)

    # â­Autoencoder ëª¨ë¸ ë¡œë”© (ì´ë¯¸ì§€ latent ì¶”ì¶œìš©)
    autoencoder = libs.autoencoder.get_model(**config.autoencoder)
    autoencoder.to(device)

    # â­CLIP ì´ë¯¸ì§€ ì¸ì½”ë” ë¡œë”© (OpenAIì˜ CLIP ëª¨ë¸)
    clip_img_model, clip_img_model_preprocess = clip.load("ViT-B/32", device=device, jit=False)

    # ë¹ˆ í…ìŠ¤íŠ¸(ê³µë°± ë¬¸ìì—´)ë¥¼ ì¸ì½”ë”©í•œ context (ì˜ˆ: unconditional generation ì‹œ ì‚¬ìš©)
    empty_context = clip_text_model.encode([''])[0]


    def split(x):   # ë²¡í„° ë¶„í•´ í•¨ìˆ˜
        C, H, W = config.z_shape
        z_dim = C * H * W
        z, clip_img = x.split([z_dim, config.clip_img_dim], dim=1)      # x ë²¡í„°ë¥¼ latent ì´ë¯¸ì§€ z ì™€ clip ì´ë¯¸ì§€ ì„ë² ë”©ìœ¼ë¡œ ë¶„í• 
        z = einops.rearrange(z, 'B (C H W) -> B C H W', C=C, H=H, W=W)      # ë²¡í„°ë¥¼ ì´ë¯¸ì§€ í…ì„œë¡œ ë³€í˜•: [B, C*H*W] â†’ [B, C, H, W]
        clip_img = einops.rearrange(clip_img, 'B (L D) -> B L D', L=1, D=config.clip_img_dim)       # clip ì„ë² ë”©ì„ [B, L*D] â†’ [B, L, D] ë¡œ ì¬êµ¬ì„± (L=1)
        return z, clip_img


    def combine(z, clip_img):
        z = einops.rearrange(z, 'B C H W -> B (C H W)')     # ì´ë¯¸ì§€ latent z ë¥¼ [B, C, H, W] â†’ [B, C*H*W]ë¡œ flatten
        clip_img = einops.rearrange(clip_img, 'B L D -> B (L D)')       # clip ì„ë² ë”©ì„ [B, L, D] â†’ [B, L*D]ë¡œ flatten
        return torch.concat([z, clip_img], dim=-1)      # ë‘ í…ì„œë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í•©ì¹˜ê¸°


    def t2i_nnet(x, timesteps, text):  # text is the low dimension version of the text clip embedding
        """
        í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ë³€í™˜ì„ ìœ„í•œ ì¡°ê±´ë¶€ ëª¨ë¸ í•¨ìˆ˜
        Classifier-Free Guidance (CFG) ë°©ì‹ìœ¼ë¡œ ì¡°ê±´ë¶€ + ë¬´ì¡°ê±´ë¶€ ì¶œë ¥ì„ í˜¼í•©í•˜ì—¬ ìƒì„± ì„±ëŠ¥ í–¥ìƒ
        1. calculate the conditional model output
        2. calculate unconditional model output
            config.sample.t2i_cfg_mode == 'empty_token': using the original cfg with the empty string
            config.sample.t2i_cfg_mode == 'true_uncond: using the unconditional model learned by our method
        3. return linear combination of conditional output and unconditional output
        """
        z, clip_img = split(x)  # ì…ë ¥ì„ ë¶„í•  (latent ì´ë¯¸ì§€, clip ì„ë² ë”©) , xëŠ” noisyí•œ image latent + img clip embedding

        t_text = torch.zeros(timesteps.size(0), dtype=torch.int, device=device) # í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í… (textëŠ” ê³ ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •)

        z_out, clip_img_out, text_out = nnet(   # ì¡°ê±´ë¶€ (text-conditioned) ëª¨ë¸ ì¶œë ¥
            z, clip_img,    # latent img, clip ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ
            text=text,
            t_img=timesteps,
            t_text=t_text,
            data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + config.data_type
        )
        x_out = combine(z_out, clip_img_out)  # [B, latent + clip_emb]ë¡œ ë‹¤ì‹œ ê²°í•©

        # guidance scaleì´ 0ì´ë©´ ì¡°ê±´ë¶€ ì¶œë ¥ë§Œ ì‚¬ìš©
        if config.sample.scale == 0.:
            return x_out

        # ë¬´ì¡°ê±´ë¶€ ì¶œë ¥ ê³„ì‚° ë°©ì‹ ì„ íƒ
        if config.sample.t2i_cfg_mode == 'empty_token':
            # ë°©ë²• 1: ë¹„ì–´ìˆëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš© (ê¸°ì¡´ CFG ë°©ì‹)
            _empty_context = einops.repeat(empty_context, 'L D -> B L D', B=x.size(0))
            if use_caption_decoder:
                _empty_context = caption_decoder.encode_prefix(_empty_context)

            z_out_uncond, clip_img_out_uncond, text_out_uncond = nnet(
                z, clip_img,
                text=_empty_context,
                t_img=timesteps,
                t_text=t_text,
                data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + config.data_type
            )
            x_out_uncond = combine(z_out_uncond, clip_img_out_uncond)

        elif config.sample.t2i_cfg_mode == 'true_uncond':
            # ë°©ë²• 2: í•™ìŠµëœ ë¬´ì¡°ê±´ë¶€ ëª¨ë¸ ì‚¬ìš©
            text_N = torch.randn_like(text)  # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¡œ í…ìŠ¤íŠ¸ ëŒ€ì²´
            z_out_uncond, clip_img_out_uncond, text_out_uncond = nnet(
                z, clip_img,
                text=text_N,
                t_img=timesteps,
                t_text=torch.ones_like(timesteps) * N,  # ìµœì¢… timestep ì‚¬ìš©
                data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + config.data_type
            )
            x_out_uncond = combine(z_out_uncond, clip_img_out_uncond)

        else:
            raise NotImplementedError  # ì •ì˜ë˜ì§€ ì•Šì€ ë°©ì‹

        # CFGë¥¼ ìœ„í•œ ìµœì¢… ì¶œë ¥ í˜¼í•©
        return x_out + config.sample.scale * (x_out - x_out_uncond)


    def i_nnet(x, timesteps):
        """
        ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”©(latent) ìƒì„±
        """
        # ì…ë ¥ xë¥¼ ì´ë¯¸ì§€ latent(z) ì™€ clip ì´ë¯¸ì§€ ì„ë² ë”©ìœ¼ë¡œ ë¶„í• 
        z, clip_img = split(x)

        # ë¬´ì‘ìœ„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±: ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ
        text = torch.randn(x.size(0), 77, config.text_dim, device=device)

        # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ìš© íƒ€ì„ìŠ¤í… (ìµœì¢… ë‹¨ê³„ì¸ N ì‚¬ìš©)
        t_text = torch.ones_like(timesteps) * N

        # nnet: ì´ë¯¸ì§€ latent, clip ì„ë² ë”©, ëœë¤ í…ìŠ¤íŠ¸ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        z_out, clip_img_out, text_out = nnet(
            z, clip_img, text=text,
            t_img=timesteps,
            t_text=t_text,
            data_type=torch.zeros_like(t_text, device=device, dtype=torch.int) + config.data_type
        )

        # zì™€ clip_imgë¥¼ ë‹¤ì‹œ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ê²°í•©í•˜ì—¬ ì¶œë ¥
        x_out = combine(z_out, clip_img_out)
        return x_out

    def t_nnet(x, timesteps):
        """
        text latentë¡œë¶€í„° ì´ë¯¸ì§€ ë³µì›
        """
        # ë¬´ì‘ìœ„ ì´ë¯¸ì§€ latent ì™€ clip ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
        z = torch.randn(x.size(0), *config.z_shape, device=device)
        clip_img = torch.randn(x.size(0), 1, config.clip_img_dim, device=device)

        # nnet í˜¸ì¶œ: í…ìŠ¤íŠ¸ ì„ë² ë”©(x)ì„ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ ë³µì› ì˜ˆì¸¡
        z_out, clip_img_out, text_out = nnet(
            z, clip_img, text=x,
            t_img=torch.ones_like(timesteps) * N,  # ì´ë¯¸ì§€ëŠ” ìµœì¢… timestep
            t_text=timesteps,  # í…ìŠ¤íŠ¸ëŠ” í˜„ì¬ timestep
            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + config.data_type
        )

        return text_out  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ë§Œ ë°˜í™˜

    def i2t_nnet(x, timesteps, z, clip_img):
        """
        ì´ë¯¸ì§€ latentì™€ clip ì„ë² ë”©ì„ ì¡°ê±´ìœ¼ë¡œ, í…ìŠ¤íŠ¸ latentë¥¼ ì˜ˆì¸¡
        1. calculate the conditional model output
        2. calculate unconditional model output
        3. return linear combination of conditional output and unconditional output
        """
        # ì´ë¯¸ì§€ ìª½ì€ timestep 0ì—ì„œ ì‹œì‘
        t_img = torch.zeros(timesteps.size(0), dtype=torch.int, device=device)

        # ì¡°ê±´ë¶€ ì˜ˆì¸¡: ì´ë¯¸ì§€(z + clip_img)ë¥¼ ì¡°ê±´ìœ¼ë¡œ í…ìŠ¤íŠ¸(x) ë³µì›
        z_out, clip_img_out, text_out = nnet(
            z, clip_img, text=x,
            t_img=t_img,
            t_text=timesteps,
            data_type=torch.zeros_like(t_img, device=device, dtype=torch.int) + config.data_type
        )

        # scaleì´ 0ì´ë©´ unconditional ì—†ì´ ì¢…ë£Œ
        if config.sample.scale == 0.:
            return text_out

        # Unconditional ì˜ˆì¸¡ìš©: ë¬´ì‘ìœ„ z, clip_imgë¡œ í•™ìŠµëœ ì§„ì§œ unconditional ëª¨ë¸ ì‚¬ìš©
        z_N = torch.randn_like(z)
        clip_img_N = torch.randn_like(clip_img)

        z_out_uncond, clip_img_out_uncond, text_out_uncond = nnet(
            z_N, clip_img_N, text=x,
            t_img=torch.ones_like(timesteps) * N,
            t_text=timesteps,
            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + config.data_type
        )

        # Classifier-free Guidance
        return text_out + config.sample.scale * (text_out - text_out_uncond)

    def split_joint(x):
        """
        í•˜ë‚˜ì˜ ê²°í•© ë²¡í„°ë¥¼ z (ì´ë¯¸ì§€ latent), clip ì´ë¯¸ì§€ ì„ë² ë”©, í…ìŠ¤íŠ¸ latentë¡œ ë¶„ë¦¬
        """
        # config.z_shape = (C, H, W)
        C, H, W = config.z_shape
        z_dim = C * H * W

        # xëŠ” z + clip_img + textê°€ ëª¨ë‘ concatëœ ìƒíƒœ
        z, clip_img, text = x.split(
            [z_dim, config.clip_img_dim, 77 * config.text_dim],
            dim=1
        )

        # ì°¨ì› ë³µì›
        z = einops.rearrange(z, 'B (C H W) -> B C H W', C=C, H=H, W=W)
        clip_img = einops.rearrange(clip_img, 'B (L D) -> B L D', L=1, D=config.clip_img_dim)
        text = einops.rearrange(text, 'B (L D) -> B L D', L=77, D=config.text_dim)

        return z, clip_img, text

    def combine_joint(z, clip_img, text):
        """z, clip ì´ë¯¸ì§€ ì„ë² ë”©, í…ìŠ¤íŠ¸ latentë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ê²°í•©"""
        z = einops.rearrange(z, 'B C H W -> B (C H W)')
        clip_img = einops.rearrange(clip_img, 'B L D -> B (L D)')
        text = einops.rearrange(text, 'B L D -> B (L D)')
        return torch.concat([z, clip_img, text], dim=-1)

    def joint_nnet(x, timesteps):
        """í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì •ë³´ê°€ ëª¨ë‘ í¬í•¨ëœ latent ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ì„¸ ê°€ì§€ ì •ë³´(z, clip_img, text)ë¥¼ ëª¨ë‘ ì˜ˆì¸¡í•˜ê³  Classifier-Free Guidanceë¡œ ì¡°ì •"""
        # z, clip ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ latentë¡œ ë¶„ë¦¬
        z, clip_img, text = split_joint(x)

        # ì¡°ê±´ë¶€ ì˜ˆì¸¡
        z_out, clip_img_out, text_out = nnet(
            z, clip_img, text=text,
            t_img=timesteps,
            t_text=timesteps,
            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + config.data_type
        )

        # ê²°ê³¼ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ê²°í•©
        x_out = combine_joint(z_out, clip_img_out, text_out)

        # scale = 0ì´ë©´ guidance ì—†ì´ return
        if config.sample.scale == 0.:
            return x_out

        # --- ì•„ë˜ë¶€í„°ëŠ” Classifier-Free Guidance (CFG) ì²˜ë¦¬ --- #

        # ì¡°ê±´ ì—†ëŠ” latent ìƒ˜í”Œ ìƒì„± (text ìœ ì§€, z/clip_img ëœë¤)
        z_noise = torch.randn(x.size(0), *config.z_shape, device=device)
        clip_img_noise = torch.randn(x.size(0), 1, config.clip_img_dim, device=device)
        text_noise = torch.randn(x.size(0), 77, config.text_dim, device=device)

        # ì²« ë²ˆì§¸ Uncond: í…ìŠ¤íŠ¸ë§Œ condition, z/clipì€ ëœë¤ (text ìœ ì§€)
        _, _, text_out_uncond = nnet(
            z_noise, clip_img_noise, text=text,
            t_img=torch.ones_like(timesteps) * N,
            t_text=timesteps,
            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + config.data_type
        )

        # ë‘ ë²ˆì§¸ Uncond: z/clipì€ ìœ ì§€, í…ìŠ¤íŠ¸ëŠ” ëœë¤
        z_out_uncond, clip_img_out_uncond, _ = nnet(
            z, clip_img, text=text_noise,
            t_img=timesteps,
            t_text=torch.ones_like(timesteps) * N,
            data_type=torch.zeros_like(timesteps, device=device, dtype=torch.int) + config.data_type
        )

        # ë‘ ê°œì˜ Uncond ì¶œë ¥ì„ ê²°í•©
        x_out_uncond = combine_joint(z_out_uncond, clip_img_out_uncond, text_out_uncond)

        # ìµœì¢…: CFGë¡œ guidance ì ìš©
        return x_out + config.sample.scale * (x_out - x_out_uncond)

    @torch.cuda.amp.autocast()
    def encode(_batch):
        """
        Autoencoderë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ latentë¡œ ì¸ì½”ë”©
        AMP(auto mixed precision) ì ìš©ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
        """
        return autoencoder.encode(_batch)

    @torch.cuda.amp.autocast()
    def decode(_batch):
        """
        latent ê³µê°„ì˜ ë²¡í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³µì›
        AMP(auto mixed precision) ì ìš©
        """
        return autoencoder.decode(_batch)

    logging.info(config.sample)
    logging.info(f'N={N}')
    """í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ê¸°ë°˜ ì…ë ¥ì— ë”°ë¼ í•„ìš”í•œ context ë²¡í„°ë“¤ì„ ìƒì„±:
    contexts: í…ìŠ¤íŠ¸ CLIP ì„ë² ë”© (77 x 768)
    img_contexts: ì´ë¯¸ì§€ì˜ VAE ì¸ì½”ë”©ëœ moment
    clip_imgs: CLIP ì´ë¯¸ì§€ ì„ë² ë”©"""
    contexts, img_contexts, clip_imgs = prepare_contexts(config, clip_text_model, clip_img_model, clip_img_model_preprocess, autoencoder)

    contexts = contexts  # the clip embedding of conditioned texts
    """contexts_low_dim: ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìš©ìœ¼ë¡œ ì°¨ì›ì„ ì¤„ì¸ í…ìŠ¤íŠ¸ context
    caption_decoderê°€ í™œì„±í™”ëœ ê²½ìš°, CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ text_dimì— ë§ì¶° ë³€í™˜"""
    #contexts_low_dim = contexts if not use_caption_decoder else caption_decoder.encode_prefix(contexts)  # the low dimensional version of the contexts, which is the input to the nnet
    # linear project ë¶ˆëŸ¬ì™”ê¸° ë•Œë¬¸ì— ìˆ˜ì •í•¨
    if linear_proj is not None:
        contexts_low_dim = linear_proj(contexts)  # [B, 77, 768] â†’ [B, 77, 64]

    img_contexts = img_contexts  # img_contexts is the autoencoder moment
    z_img = autoencoder.sample(img_contexts)
    clip_imgs = clip_imgs  # the clip embedding of conditioned image

    if config.mode in ['t2i', 't2i2t']:
        _n_samples = contexts_low_dim.size(0)   # í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ë©´ í…ìŠ¤íŠ¸ context ê°œìˆ˜ ê¸°ì¤€
    elif config.mode in ['i2t', 'i2t2i']:
        _n_samples = img_contexts.size(0)   # ì´ë¯¸ì§€ ê¸°ë°˜ì´ë©´ ì´ë¯¸ì§€ context ê¸°ì¤€
    else:
        _n_samples = config.n_samples   # ì•„ë‹ˆë©´ ê¸°ë³¸ ì„¤ì •ê°’(config.n_samples) ì‚¬ìš©


    def sample_fn(mode, **kwargs):
        # ì´ˆê¸° ë…¸ì´ì¦ˆ í…ì„œ ìƒì„±: ìƒ˜í”Œ ìˆ˜ë§Œí¼ z latent, clip ì´ë¯¸ì§€ ì„ë² ë”©, í…ìŠ¤íŠ¸ ì„ë² ë”© ì´ˆê¸°í™”
        _z_init = torch.randn(_n_samples, *config.z_shape, device=device)  # VAE latent ê³µê°„ ë…¸ì´ì¦ˆ
        _clip_img_init = torch.randn(_n_samples, 1, config.clip_img_dim, device=device)  # CLIP ì´ë¯¸ì§€ ì„ë² ë”© ë…¸ì´ì¦ˆ
        _text_init = torch.randn(_n_samples, 77, config.text_dim, device=device)  # í…ìŠ¤íŠ¸ ì„ë² ë”© ë…¸ì´ì¦ˆ (77ì€ CLIP í† í° ìˆ˜)
        # ì„ íƒ ëª¨ë“œì— ë”°ë¼ ì´ˆê¸° ì…ë ¥ ë²¡í„°ë¥¼ êµ¬ì„±
        if mode == 'joint':
            _x_init = combine_joint(_z_init, _clip_img_init, _text_init)
        elif mode in ['t2i', 'i']:
            _x_init = combine(_z_init, _clip_img_init)
        elif mode in ['i2t', 't']:
            _x_init = _text_init
        # ë² íƒ€ ìŠ¤ì¼€ì¤„ ê¸°ë°˜ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (DPM-Solverì—ì„œ ì‚¬ìš©)
        noise_schedule = NoiseScheduleVP(schedule='discrete', betas=torch.tensor(_betas, device=device).float())

        # ë””í“¨ì „ ëª¨ë¸ í•¨ìˆ˜ ì •ì˜: ì‹œê°„ tì— ë”°ë¼ ì ì ˆí•œ ë„¤íŠ¸ì›Œí¬ í•¨ìˆ˜ í˜¸ì¶œ
        def model_fn(x, t_continuous):
            t = t_continuous * N  # ì—°ì† ì‹œê°„ê°’ì„ ì •ìˆ˜ timestepìœ¼ë¡œ ë³€í™˜
            if mode == 'joint':
                return joint_nnet(x, t)
            elif mode == 't2i':
                return t2i_nnet(x, t, **kwargs)
            elif mode == 'i2t':
                return i2t_nnet(x, t, **kwargs)
            elif mode == 'i':
                return i_nnet(x, t)
            elif mode == 't':
                return t_nnet(x, t)

        # DPM-Solver ê°ì²´ ìƒì„±: ê³ ì† ìƒ˜í”Œë§ì„ ìœ„í•œ ë””í“¨ì „ ìƒ˜í”ŒëŸ¬
        dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=True, thresholding=False)
        with torch.no_grad():
            with torch.autocast(device_type=device):
                start_time = time.time()
                """
                x = dpm_solver.sample(
                    _x_init,                               # ì´ˆê¸° ë…¸ì´ì¦ˆ
                    steps=config.sample.sample_steps,     # ìƒ˜í”Œë§ ìŠ¤í… ìˆ˜
                    eps=1. / N,                            # ì´ˆê¸° ì‹œê°„
                    T=1.                                   # ìµœì¢… ì‹œê°„
                )
                """
                x = dpm_solver.sample(_x_init, steps=config.sample.sample_steps, eps=1. / N, T=1.)
                end_time = time.time()
                print(f'\ngenerate {_n_samples} samples with {config.sample.sample_steps} steps takes {end_time - start_time:.2f}s')

        os.makedirs(config.output_path, exist_ok=True)
        if mode == 'joint':
            _z, _clip_img, _text = split_joint(x)
            return _z, _clip_img, _text
        elif mode in ['t2i', 'i']:
            _z, _clip_img = split(x)
            return _z, _clip_img
        elif mode in ['i2t', 't']:
            return x

    def watermarking(save_path):
        img_pre = Image.open(save_path)
        img_pos = utils.add_water(img_pre)
        img_pos.save(save_path)

    if config.mode in ['joint']:
        _z, _clip_img, _text = sample_fn(config.mode)  # ìƒ˜í”Œë§ í•¨ìˆ˜ í˜¸ì¶œí•´ì„œ latent ë³€ìˆ˜ë“¤ ìƒì„±
        samples = unpreprocess(decode(_z))  # VAE ë””ì½”ë”© í›„ ì´ë¯¸ì§€ í˜•íƒœë¡œ ë³€í™˜
        prompts = caption_decoder.generate_captions(_text)  # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ìº¡ì…˜ ìƒì„±

        os.makedirs(os.path.join(config.output_path, config.mode), exist_ok=True)
        # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë“¤ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        with open(os.path.join(config.output_path, config.mode, 'prompts.txt'), 'w') as f:
            print('\n'.join(prompts), file=f)

        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì”© ì €ì¥í•˜ê³  ì›Œí„°ë§ˆí¬ ì¶”ê°€
        for idx, sample in enumerate(samples):
            save_path = os.path.join(config.output_path, config.mode, f'{idx}.png')
            save_image(sample, save_path)
            watermarking(save_path)

    elif config.mode in ['t2i', 'i', 'i2t2i']:
        if config.mode == 't2i':
            _z, _clip_img = sample_fn(config.mode, text=contexts_low_dim)  # conditioned on the text embedding
        elif config.mode == 'i':
            _z, _clip_img = sample_fn(config.mode)
        elif config.mode == 'i2t2i':
            # ì´ë¯¸ì§€->í…ìŠ¤íŠ¸->ì´ë¯¸ì§€: ë¨¼ì € í…ìŠ¤íŠ¸ ìƒì„± í›„ ê·¸ê±¸ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
            _text = sample_fn('i2t', z=z_img, clip_img=clip_imgs)  # conditioned on the image embedding
            _z, _clip_img = sample_fn('t2i', text=_text)

        samples = unpreprocess(decode(_z))  # latent decode í›„ ì´ë¯¸ì§€ ë³€í™˜
        os.makedirs(os.path.join(config.output_path, config.mode), exist_ok=True)

        # ê°œë³„ ì´ë¯¸ì§€ ì €ì¥ ë° ì›Œí„°ë§ˆí¬ ì¶”ê°€
        for idx, sample in enumerate(samples):
            save_path = os.path.join(config.output_path, config.mode, f'{idx}.png')
            save_image(sample, save_path)
            watermarking(save_path)
        # save a grid of generated images # ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ì„ ëª¨ì•„ì„œ ê·¸ë¦¬ë“œ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥ (ì›Œí„°ë§ˆí¬ í¬í•¨)
        samples_pos = []
        for idx, sample in enumerate(samples):
            sample_pil = standard_transforms.ToPILImage()(sample)
            sample_pil = utils.add_water(sample_pil)
            sample = standard_transforms.ToTensor()(sample_pil)
            samples_pos.append(sample)
        samples = make_grid(samples_pos, config.nrow)
        save_path = os.path.join(config.output_path, config.mode, f'grid.png')
        save_image(samples, save_path)


    elif config.mode in ['i2t', 't', 't2i2t']:
        if config.mode == 'i2t':
            _text = sample_fn(config.mode, z=z_img, clip_img=clip_imgs)  # conditioned on the image embedding
        elif config.mode == 't':
            _text = sample_fn(config.mode)
        elif config.mode == 't2i2t':
            _z, _clip_img = sample_fn('t2i', text=contexts_low_dim)
            _text = sample_fn('i2t', z=_z, clip_img=_clip_img)

        # í…ìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œë¶€í„° ì‹¤ì œ ë¬¸ì¥ ìƒì„±
        samples = caption_decoder.generate_captions(_text)
        logging.info(samples)
        os.makedirs(os.path.join(config.output_path, config.mode), exist_ok=True)
        with open(os.path.join(config.output_path, config.mode, f'{config.mode}.txt'), 'w') as f:
            print('\n'.join(samples), file=f)

    print(f'\nGPU memory usage: {torch.cuda.max_memory_reserved() / 1024 ** 3:.2f} GB')
    print(f'\nresults are saved in {os.path.join(config.output_path, config.mode)} :)')


from absl import flags
from absl import app
from ml_collections import config_flags
import os


FLAGS = flags.FLAGS
config_flags.DEFINE_config_file(
    "config", "configs/sample_unidiffuser_v1.py", "Configuration.", lock_config=False)
#flags.DEFINE_string("nnet_path", "models/uvit_v1.pth", "The nnet to evaluate.")
flags.DEFINE_string("nnet_path", "/home/ivpl-d29/sichoi/Emo/unidiffuser/si_finetuning_2/checkpoint_step_4000.pth", "The nnet to evaluate.")
flags.DEFINE_string("output_path", "out", "dir to write results to")
flags.DEFINE_string("prompt", "an elephant under the sea", "the prompt for text-to-image generation and text variation")
flags.DEFINE_string("img", "assets/space.jpg", "the image path for image-to-text generation and image variation")
flags.DEFINE_integer("n_samples", 1, "the number of samples to generate")
flags.DEFINE_integer("nrow", 4, "number of images displayed in each row of the grid")
flags.DEFINE_string("mode", None,
                    "type of generation, one of t2i / i2t / joint / i / t / i2t2i/ t2i2t\n"
                    "t2i: text to image\n"
                    "i2t: image to text\n"
                    "joint: joint generation of text and image\n"
                    "i: only generate image\n"
                    "t: only generate text\n"
                    "i2t2i: image variation, first image to text, then text to image\n"
                    "t2i2t: text variation, first text to image, the image to text\n"
                    )


def main(argv):
    config = FLAGS.config
    config.nnet_path = FLAGS.nnet_path
    config.output_path = FLAGS.output_path
    config.prompt = FLAGS.prompt
    config.nrow = min(FLAGS.nrow, FLAGS.n_samples)
    config.img = FLAGS.img
    config.n_samples = FLAGS.n_samples
    config.mode = FLAGS.mode

    evaluate(config)


if __name__ == "__main__":
    app.run(main)
